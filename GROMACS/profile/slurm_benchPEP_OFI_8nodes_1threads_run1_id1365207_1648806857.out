CrayPat/X:  Version 21.09.0 Revision b02949528  08/17/21 03:15:42
                      :-) GROMACS - gmx mdrun, 2021.5 (-:

                            GROMACS is written by:
     Andrey Alekseenko              Emile Apol              Rossen Apostolov     
         Paul Bauer           Herman J.C. Berendsen           Par Bjelkmar       
       Christian Blau           Viacheslav Bolnykh             Kevin Boyd        
     Aldert van Buuren           Rudi van Drunen             Anton Feenstra      
    Gilles Gouaillardet             Alan Gray               Gerrit Groenhof      
       Anca Hamuraru            Vincent Hindriksen          M. Eric Irrgang      
      Aleksei Iupinov           Christoph Junghans             Joe Jordan        
    Dimitrios Karkoulis            Peter Kasson                Jiri Kraus        
      Carsten Kutzner              Per Larsson              Justin A. Lemkul     
       Viveca Lindahl            Magnus Lundborg             Erik Marklund       
        Pascal Merz             Pieter Meulenhoff            Teemu Murtola       
        Szilard Pall               Sander Pronk              Roland Schulz       
       Michael Shirts            Alexey Shvetsov             Alfons Sijbers      
       Peter Tieleman              Jon Vincent              Teemu Virolainen     
     Christian Wennberg            Maarten Wolf              Artem Zhmurov       
                           and the project leaders:
        Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel

Copyright (c) 1991-2000, University of Groningen, The Netherlands.
Copyright (c) 2001-2019, The GROMACS development team at
Uppsala University, Stockholm University and
the Royal Institute of Technology, Sweden.
check out http://www.gromacs.org for more information.

GROMACS is free software; you can redistribute it and/or modify it
under the terms of the GNU Lesser General Public License
as published by the Free Software Foundation; either version 2.1
of the License, or (at your option) any later version.

GROMACS:      gmx mdrun, version 2021.5
Executable:   /work/z19/z19/wlucas/cug22-bench/sw/gromacs/2021.5-patlite/bin/gmx_mpi
Data prefix:  /work/z19/z19/wlucas/cug22-bench/sw/gromacs/2021.5-patlite
Working dir:  /mnt/lustre/a2fs-work2/work/z19/z19/wlucas/cug22-bench/gromacs-benchPEP/pat-nnodes-8-ntpt1
Command line:
  gmx_mpi mdrun -s ../benchPEP.tpr -g benchPEP_OFI_8nodes_1threads_run1_id1365207_1648806857.log -nsteps 10000 -noconfout

Reading file ../benchPEP.tpr, VERSION 4.6.2-dev-20130529-36d170c (single precision)
Note: file tpx version 83, software tpx version 122
Overriding nsteps with value passed on the command line: 10000 steps, 20 ps
Changing nstlist from 40 to 80, rlist from 1.283 to 1.324


Using 1024 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Structure    371 generated by disco in water'
10000 steps,     20.0 ps.


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 2.8%.
 The balanceable part of the MD step is 88%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 2.5%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 % Z 0 %
 Average PME mesh/force load: 0.578
 Part of the total run time spent waiting due to PP/PME imbalance: 9.5 %

NOTE: 9.5 % performance was lost because the PME ranks
      had less work to do than the PP ranks.
      You might want to decrease the number of PME ranks
      or decrease the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:   233683.846      228.219   102394.7
                 (ns/day)    (hour/ns)
Performance:        7.572        3.169

GROMACS reminds you: "It's easy to remember: a half a kT is equal to five fourths of a kJ/mol." (Anders Gabrielsson)


#################################################################
#                                                               #
#            CrayPat-lite Performance Statistics                #
#                                                               #
#################################################################

CrayPat/X:  Version 21.09.0 Revision b02949528  08/17/21 03:15:42
Experiment:                   lite  lite-samples  
Number of PEs (MPI ranks):   1,024
Numbers of PEs per Node:       128  PEs on each of  8  Nodes
Numbers of Threads per PE:       1
Number of Cores per Socket:     64
Execution start time:  Fri Apr  1 10:54:24 2022
System name and speed:  nid002159  2.250 GHz (nominal)
AMD   Rome                 CPU  Family: 23  Model: 49  Stepping:  0
Core Performance Boost:  All 1024 PEs have CPB capability



Avg Process Time:    252.71 secs              
High Memory:      135,974.4 MiBytes     132.8 MiBytes per PE
I/O Write Rate:    0.068291 MiBytes/sec       

Notes for table 1:

  This table shows functions that have significant exclusive sample
    hits, averaged across ranks.
  For further explanation, see the "General table notes" below,
    or use:  pat_report -v -O samp_profile ...

Table 1:  Profile by Function

  Samp% |     Samp |    Imb. |  Imb. | Group
        |          |    Samp | Samp% |  Function=[MAX10]
        |          |         |       |   PE=HIDE
       
 100.0% | 25,149.8 |      -- |    -- | Total
|---------------------------------------------------------------------------
|  68.1% | 17,139.2 |      -- |    -- | USER
||--------------------------------------------------------------------------
||  55.1% | 13,862.0 | 4,792.0 | 25.7% | nbnxm_kernel_ElecEw_VdwLJCombLB_F_4xm
||   2.4% |    600.3 | 1,881.7 | 75.9% | spread_on_grid
||   1.7% |    415.4 | 1,340.6 | 76.4% | gather_f_bsplines
||   1.5% |    365.0 |   171.0 | 31.9% | nbnxn_kernel_prune_4xn
||==========================================================================
|  31.4% |  7,898.4 |      -- |    -- | MPI
||--------------------------------------------------------------------------
||  12.7% |  3,193.3 | 7,138.7 | 69.2% | MPI_Waitall
||   5.0% |  1,257.1 |   373.9 | 22.9% | MPI_Sendrecv
||   3.4% |    859.3 | 1,060.7 | 55.3% | MPI_Bcast
||   3.2% |    803.9 | 2,258.1 | 73.8% | MPI_Recv
||   2.8% |    707.9 | 2,235.1 | 76.0% | MPI_Alltoall
||   2.5% |    640.5 | 1,060.5 | 62.4% | MPI_Scatterv
|===========================================================================

Notes for table 2:

  This table shows functions, and line numbers within functions, that
    have significant exclusive sample hits, averaged across ranks.
  For further explanation, see the "General table notes" below,
    or use:  pat_report -v -O samp_profile+src ...

Table 2:  Profile by Group, Function, and Line

  Samp% |     Samp |    Imb. |  Imb. | Group
        |          |    Samp | Samp% |  Function=[MAX10]
        |          |         |       |   Source
        |          |         |       |    Line
        |          |         |       |     PE=HIDE
       
 100.0% | 25,149.8 |      -- |    -- | Total
|-----------------------------------------------------------------------------
|  68.1% | 17,139.2 |      -- |    -- | USER
||----------------------------------------------------------------------------
||  55.1% | 13,862.0 |      -- |    -- | nbnxm_kernel_ElecEw_VdwLJCombLB_F_4xm
|||---------------------------------------------------------------------------
3||  16.0% |  4,022.6 |      -- |    -- | gromacs/nbnxm/kernels_simd_4xm/kernel_inner.h
||||--------------------------------------------------------------------------
4|||   4.6% |  1,167.1 |   645.9 | 35.7% | line.401
4|||   2.4% |    602.4 |   292.6 | 32.7% | line.740
4|||   1.1% |    269.5 |   190.5 | 41.5% | line.1185
4|||   2.0% |    498.8 |   248.2 | 33.3% | line.1186
||||==========================================================================
3||  13.6% |  3,419.4 |      -- |    -- | x86_64-suse-linux/11.2.0/include/avxintrin.h
||||--------------------------------------------------------------------------
4|||   2.6% |    652.4 |   299.6 | 31.5% | line.149
4|||   1.6% |    405.6 |   208.4 | 34.0% | line.174
4|||   4.0% |    996.1 |   453.9 | 31.3% | line.320
4|||   1.9% |    489.1 |   226.9 | 31.7% | line.368
4|||   1.7% |    438.5 |   218.5 | 33.3% | line.1041
||||==========================================================================
3||  12.0% |  3,024.6 |      -- |    -- | x86_64-suse-linux/11.2.0/include/fmaintrin.h
4||  12.0% |  3,008.2 | 1,183.8 | 28.3% |  line.65
3||   8.0% |  2,006.8 |      -- |    -- | gromacs/simd/impl_x86_avx_256/impl_x86_avx_256_simd_float.h
4||   7.3% |  1,845.9 |   756.1 | 29.1% |  line.192
3||   3.8% |    959.0 |      -- |    -- | src/gromacs/simd/simd_math.h
4||   1.0% |    246.4 |   136.6 | 35.7% |  line.129
|||===========================================================================
||   2.4% |    600.3 |      -- |    -- | spread_on_grid
3|   1.4% |    349.2 |      -- |    -- |  src/gromacs/ewald/pme_spread.cpp
||   1.7% |    415.4 |      -- |    -- | gather_f_bsplines
3|   1.1% |    267.1 |      -- |    -- |  src/gromacs/ewald/pme_gather.cpp
||   1.5% |    365.0 |      -- |    -- | nbnxn_kernel_prune_4xn
||============================================================================
|  31.4% |  7,898.4 |      -- |    -- | MPI
||----------------------------------------------------------------------------
||  12.7% |  3,193.3 | 7,138.7 | 69.2% | MPI_Waitall
||   5.0% |  1,257.1 |   373.9 | 22.9% | MPI_Sendrecv
||   3.4% |    859.3 | 1,060.7 | 55.3% | MPI_Bcast
||   3.2% |    803.9 | 2,258.1 | 73.8% | MPI_Recv
||   2.8% |    707.9 | 2,235.1 | 76.0% | MPI_Alltoall
||   2.5% |    640.5 | 1,060.5 | 62.4% | MPI_Scatterv
|=============================================================================

Observation:  MPI Grid Detection

    There appears to be point-to-point MPI communication in a 256 X 2 X
    2 grid pattern. The 31.4% of the total execution time spent in MPI
    functions might be reduced with a rank order that maximizes
    communication between ranks on the same node. The effect of several
    rank orders is estimated below.

    A file named MPICH_RANK_ORDER.Grid was generated along with this
    report and contains usage instructions and the Custom rank order
    from the following table.

    Rank Order    On-Node    On-Node  MPICH_RANK_REORDER_METHOD
                 Bytes/PE  Bytes/PE%  
                            of Total  
                            Bytes/PE  

        Custom  6.721e+12     87.24%  3
           SMP  6.590e+12     85.54%  1
          Fold  2.184e+12     28.34%  2
    RoundRobin  1.894e+12     24.59%  0


Observation:  MPI utilization

    The time spent processing MPI communications is relatively high. 
    Functions and callsites responsible for consuming the most time can
    be found in the table generated by pat_report -O callers+src (within
    the MPI group).


Notes for table 3:

  This table shows memory traffic for numa nodes, taking for each numa
    node the maximum value across nodes. It also shows the balance in
    memory traffic by showing the top 3 and bottom 3 node values.
  For further explanation, see the "General table notes" below,
    or use:  pat_report -v -O mem_bw ...

Table 3:  Memory Bandwidth by Numanode

   Memory |     Read |    Write |     Thread |  Memory |  Memory | Numanode
  Traffic |   Memory |   Memory |       Time | Traffic | Traffic |  Node Id=[max3,min3]
   GBytes |  Traffic |  Traffic |            |  GBytes |       / |   PE=HIDE
          |   GBytes |   GBytes |            |   / Sec | Nominal | 
          |          |          |            |         |    Peak | 
|-----------------------------------------------------------------------------
|  1881.84 |  1781.50 |   100.34 | 252.655130 |    7.45 |    3.6% | numanode.0
||----------------------------------------------------------------------------
||  1901.61 |  1799.81 |   101.80 | 252.644931 |    7.53 |    3.7% | nid.2159
||  1896.07 |  1810.46 |    85.62 | 252.647909 |    7.50 |    3.7% | nid.2242
||  1894.35 |  1812.26 |    82.09 | 252.651746 |    7.50 |    3.7% | nid.2256
||  1890.16 |  1802.15 |    88.01 | 252.639618 |    7.48 |    3.7% | nid.2255
||  1880.69 |  1796.91 |    83.78 | 252.645294 |    7.44 |    3.6% | nid.2183
||  1875.34 |  1789.71 |    85.63 | 252.655130 |    7.42 |    3.6% | nid.2182
||============================================================================
|  1891.75 |  1805.90 |    85.86 | 252.652524 |    7.49 |    3.7% | numanode.1
||----------------------------------------------------------------------------
||  1906.72 |  1819.25 |    87.47 | 252.649856 |    7.55 |    3.7% | nid.2159
||  1897.36 |  1815.54 |    81.82 | 252.644289 |    7.51 |    3.7% | nid.2242
||  1893.03 |  1808.56 |    84.48 | 252.647566 |    7.49 |    3.7% | nid.2243
||  1888.05 |  1806.25 |    81.80 | 252.645610 |    7.47 |    3.6% | nid.2256
||  1886.82 |  1802.97 |    83.85 | 252.652524 |    7.47 |    3.6% | nid.2255
||  1885.19 |  1802.91 |    82.27 | 252.642020 |    7.46 |    3.6% | nid.2182
||============================================================================
|  1903.34 |  1802.82 |   100.52 | 252.672304 |    7.53 |    3.7% | numanode.2
||----------------------------------------------------------------------------
||  1903.34 |  1802.82 |   100.52 | 252.652421 |    7.53 |    3.7% | nid.2256
||  1894.23 |  1803.57 |    90.67 | 252.642076 |    7.50 |    3.7% | nid.2243
||  1887.87 |  1806.16 |    81.71 | 252.647599 |    7.47 |    3.6% | nid.2183
||  1880.58 |  1799.01 |    81.58 | 252.646924 |    7.44 |    3.6% | nid.2242
||  1878.11 |  1793.35 |    84.75 | 252.651487 |    7.43 |    3.6% | nid.2181
||  1875.08 |  1793.88 |    81.20 | 252.647547 |    7.42 |    3.6% | nid.2159
||============================================================================
|  1961.46 |  1812.05 |   149.41 | 252.647652 |    7.76 |    3.8% | numanode.3
||----------------------------------------------------------------------------
||  1974.72 |  1824.09 |   150.64 | 252.646068 |    7.82 |    3.8% | nid.2183
||  1922.78 |  1818.80 |   103.98 | 252.641431 |    7.61 |    3.7% | nid.2182
||  1908.24 |  1817.15 |    91.10 | 252.645786 |    7.55 |    3.7% | nid.2181
||  1894.96 |  1812.54 |    82.42 | 252.644369 |    7.50 |    3.7% | nid.2243
||  1893.89 |  1799.62 |    94.27 | 252.641431 |    7.50 |    3.7% | nid.2255
||  1893.48 |  1808.52 |    84.97 | 252.647652 |    7.49 |    3.7% | nid.2159
||============================================================================
|  1889.50 |  1799.52 |    89.98 | 252.666547 |    7.48 |    3.7% | numanode.4
||----------------------------------------------------------------------------
||  1900.37 |  1808.25 |    92.12 | 252.657760 |    7.52 |    3.7% | nid.2242
||  1896.89 |  1813.76 |    83.13 | 252.666547 |    7.51 |    3.7% | nid.2256
||  1896.43 |  1813.19 |    83.24 | 252.658610 |    7.51 |    3.7% | nid.2183
||  1888.98 |  1807.16 |    81.82 | 252.662587 |    7.48 |    3.7% | nid.2159
||  1882.70 |  1797.99 |    84.72 | 252.659576 |    7.45 |    3.6% | nid.2255
||  1878.45 |  1791.48 |    86.97 | 252.655437 |    7.43 |    3.6% | nid.2243
||============================================================================
|  1894.20 |  1811.11 |    83.09 | 252.671476 |    7.50 |    3.7% | numanode.5
||----------------------------------------------------------------------------
||  1917.95 |  1834.26 |    83.69 | 252.661464 |    7.59 |    3.7% | nid.2182
||  1902.44 |  1820.76 |    81.69 | 252.654883 |    7.53 |    3.7% | nid.2183
||  1902.20 |  1819.16 |    83.04 | 252.666062 |    7.53 |    3.7% | nid.2181
||  1887.22 |  1803.73 |    83.49 | 252.662366 |    7.47 |    3.6% | nid.2159
||  1880.18 |  1794.32 |    85.86 | 252.660238 |    7.44 |    3.6% | nid.2255
||  1875.23 |  1790.42 |    84.81 | 252.671476 |    7.42 |    3.6% | nid.2243
||============================================================================
|  1884.95 |  1803.58 |    81.37 | 252.661405 |    7.46 |    3.6% | numanode.6
||----------------------------------------------------------------------------
||  1907.46 |  1822.78 |    84.69 | 252.653302 |    7.55 |    3.7% | nid.2242
||  1904.10 |  1823.30 |    80.80 | 252.656920 |    7.54 |    3.7% | nid.2181
||  1901.54 |  1817.31 |    84.23 | 252.661405 |    7.53 |    3.7% | nid.2159
||  1888.42 |  1803.52 |    84.90 | 252.661106 |    7.47 |    3.6% | nid.2255
||  1887.62 |  1806.64 |    80.98 | 252.658334 |    7.47 |    3.6% | nid.2183
||  1882.53 |  1799.20 |    83.33 | 252.657716 |    7.45 |    3.6% | nid.2243
||============================================================================
|  1926.36 |  1824.31 |   102.05 | 252.664659 |    7.62 |    3.7% | numanode.7
||----------------------------------------------------------------------------
||  1922.83 |  1836.33 |    86.50 | 252.653600 |    7.61 |    3.7% | nid.2159
||  1922.69 |  1819.96 |   102.73 | 252.652341 |    7.61 |    3.7% | nid.2256
||  1916.86 |  1832.76 |    84.10 | 252.647283 |    7.59 |    3.7% | nid.2181
||  1904.43 |  1820.74 |    83.69 | 252.664659 |    7.54 |    3.7% | nid.2182
||  1904.28 |  1816.21 |    88.07 | 252.660910 |    7.54 |    3.7% | nid.2242
||  1892.76 |  1807.14 |    85.63 | 252.652795 |    7.49 |    3.7% | nid.2243
|=============================================================================

Notes for table 4:

  This table shows energy and power usage for the nodes with the
    maximum, mean, and minimum usage, as well as the sum of usage over
    all nodes.
    Energy and power for accelerators is also shown, if applicable.
  For further explanation, see the "General table notes" below,
    or use:  pat_report -v -O program_energy ...

Table 4:  Program energy and power usage (from Cray PM)

      Node |      Node |    Process | Node Id=[mmm]
    Energy | Power (W) |       Time |  PE=HIDE
       (J) |           |            | 
          
 1,188,046 | 4,701.219 | 252.710176 | Total
|--------------------------------------------------
|   151,960 |   601.326 | 252.708228 | nid.2183
|   148,133 |   586.177 | 252.710354 | nid.2243
|   145,463 |   575.614 | 252.709281 | nid.2181
|==================================================

Notes for table 5:

  This table show the average time and number of bytes written to each
    output file, taking the average over the number of ranks that
    wrote to the file.  It also shows the number of write operations,
    and average rates.
  For further explanation, see the "General table notes" below,
    or use:  pat_report -v -O write_stats ...

Table 5:  File Output Stats by Filename

      Avg |      Avg |  Write Rate | Number |       Avg | Bytes/ | File Name=!x/^/(proc|sys)/
    Write |    Write | MiBytes/sec |     of |    Writes |   Call |  PE=HIDE
 Time per |  MiBytes |             | Writer |       per |        | 
   Writer |      per |             |  Ranks |    Writer |        | 
     Rank |   Writer |             |        |      Rank |        | 
          |     Rank |             |        |           |        | 
|-----------------------------------------------------------------------------
| 0.162842 | 0.008491 |    0.052144 |  1,024 | 220,719.9 |   0.04 | _UnknownFile_
| 0.000526 | 0.026520 |   50.453314 |      1 |   1,186.0 |  23.45 | benchPEP_OFI_8nodes_1threads_run1_id1365207_1648806857.log
| 0.000237 | 0.002616 |   11.027827 |  1,024 |      40.2 |  68.27 | /dev/infiniband/rdma_cm
| 0.000231 | 0.003770 |   16.318814 |      1 |     132.0 |  29.95 | stderr
|=============================================================================

Table 6:  Lustre File Information

                                                                                     File Path |    Stripe | Stripe | Stripe | OST list
                                                                                               |      size | offset |  count | 
------------------------------------------------------------------------------
                        /work/z19/z19/wlucas/cug22-bench/sw/gromacs/2021.5-patlite/bin/gmx_mpi | 1,048,576 |      0 |      1 | 0
 /work/z19/z19/wlucas/cug22-bench/sw/gromacs/2021.5-patlite/share/gromacs/top/residuetypes.dat | 1,048,576 |      0 |      1 | 9
                                                                               ../benchPEP.tpr | 1,048,576 |      0 |      1 | 1
                                    benchPEP_OFI_8nodes_1threads_run1_id1365207_1648806857.log | 1,048,576 |      0 |      1 | 7
                                                                                      ener.edr | 1,048,576 |      0 |      1 | 3
                                                                                ./#ener.edr.1# | 1,048,576 |      0 |      1 | 6
==============================================================================
Program invocation:
  /work/z19/z19/wlucas/cug22-bench/sw/gromacs/2021.5-patlite/bin/gmx_mpi mdrun -s ../benchPEP.tpr -g benchPEP_OFI_8nodes_1threads_run1_id1365207_1648806857.log -nsteps 10000 -noconfout

For a complete report with expanded tables and notes, run:
  pat_report /mnt/lustre/a2fs-work2/work/z19/z19/wlucas/cug22-bench/gromacs-benchPEP/pat-nnodes-8-ntpt1/gmx_mpi+89113-2159s

For help identifying callers of particular functions:
  pat_report -O callers+src /mnt/lustre/a2fs-work2/work/z19/z19/wlucas/cug22-bench/gromacs-benchPEP/pat-nnodes-8-ntpt1/gmx_mpi+89113-2159s
To see the entire call tree:
  pat_report -O calltree+src /mnt/lustre/a2fs-work2/work/z19/z19/wlucas/cug22-bench/gromacs-benchPEP/pat-nnodes-8-ntpt1/gmx_mpi+89113-2159s

For interactive, graphical performance analysis, run:
  app2 /mnt/lustre/a2fs-work2/work/z19/z19/wlucas/cug22-bench/gromacs-benchPEP/pat-nnodes-8-ntpt1/gmx_mpi+89113-2159s

================  End of CrayPat-lite output  ==========================
